{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58d10a1",
   "metadata": {},
   "source": [
    "# Interactive Spark Session on Synapse Spark Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f134ff",
   "metadata": {},
   "source": [
    "### Install the azureml-synapse package on your compute instance.\n",
    "\n",
    "We will pip install the package *azureml-synapse*, but note if you chose to use jupyter lab, then you would need to additionally need to run the command: *!jupyter lab build --minimize=False*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b6e57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azureml-synapse\n",
      "  Downloading azureml_synapse-1.34.0-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting azure.identity>=1.4.1\n",
      "  Downloading azure_identity-1.6.1-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 82.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: azureml-core~=1.34.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-synapse) (1.34.0)\n",
      "Collecting sparkmagic<=0.16.0,>=0.15.0\n",
      "  Downloading sparkmagic-0.16.0.tar.gz (35 kB)\n",
      "Collecting azure.mgmt.synapse>=2.0.0\n",
      "  Downloading azure_mgmt_synapse-2.0.0-py2.py3-none-any.whl (442 kB)\n",
      "\u001b[K     |████████████████████████████████| 442 kB 68.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hdijupyterutils<=0.16.0,>=0.15.0\n",
      "  Downloading hdijupyterutils-0.16.0.tar.gz (4.6 kB)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.1.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure.identity>=1.4.1->azureml-synapse) (3.4.8)\n",
      "Requirement already satisfied, skipping upgrade: azure-core<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure.identity>=1.4.1->azureml-synapse) (1.18.0)\n",
      "Requirement already satisfied, skipping upgrade: msal<2.0.0,>=1.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure.identity>=1.4.1->azureml-synapse) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure.identity>=1.4.1->azureml-synapse) (1.16.0)\n",
      "Collecting msal-extensions~=0.3.0\n",
      "  Downloading msal_extensions-0.3.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyopenssl<21.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (20.0.1)\n",
      "Requirement already satisfied, skipping upgrade: msrestazure<=0.6.4,>=0.4.33 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.6.4)\n",
      "Requirement already satisfied, skipping upgrade: azure-graphrbac<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.61.1)\n",
      "Requirement already satisfied, skipping upgrade: ndg-httpsclient<=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-containerregistry>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (8.1.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<=1.26.6,>=1.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: azure-common<2.0.0,>=1.1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (1.1.27)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: msrest<1.0.0,>=0.5.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.6.21)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml<0.17.5,>=0.15.35 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.17.4)\n",
      "Requirement already satisfied, skipping upgrade: docker<6.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (5.0.2)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-storage<16.0.0,>=1.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (11.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonpickle<3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pathspec<1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: backports.tempfile in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (1.0)\n",
      "Requirement already satisfied, skipping upgrade: contextlib2<22.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (21.6.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.7.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: SecretStorage<4.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (3.3.1)\n",
      "Requirement already satisfied, skipping upgrade: adal<=1.2.7,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (1.2.7)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (2021.1)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-authorization<1.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (0.61.0)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-resource<15.0.0,>=1.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (13.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.19.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (2.26.0)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-keyvault<10.0.0,>=0.40.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azureml-core~=1.34.0->azureml-synapse) (9.1.0)\n",
      "Collecting autovizwidget>=0.6\n",
      "  Downloading autovizwidget-0.19.1.tar.gz (8.7 kB)\n",
      "Requirement already satisfied, skipping upgrade: ipython>=4.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (7.16.1)\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 78.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: mock in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=0.17.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.25.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: ipykernel in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (5.5.5)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets>5.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (7.6.4)\n",
      "Requirement already satisfied, skipping upgrade: notebook>=4.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (6.4.3)\n",
      "Requirement already satisfied, skipping upgrade: tornado>=4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (6.1)\n",
      "Collecting requests_kerberos>=0.8.0\n",
      "  Downloading requests_kerberos-0.12.0-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: azure-mgmt-core<2.0.0,>=1.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from azure.mgmt.synapse>=2.0.0->azureml-synapse) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyter>=1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from hdijupyterutils<=0.16.0,>=0.15.0->azureml-synapse) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.12 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cryptography>=2.1.4->azure.identity>=1.4.1->azureml-synapse) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: portalocker~=1.0; platform_system != \"Windows\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msal-extensions~=0.3.0->azure.identity>=1.4.1->azureml-synapse) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ndg-httpsclient<=0.5.1->azureml-core~=1.34.0->azureml-synapse) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.34.0->azureml-synapse) (2021.5.30)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.34.0->azureml-synapse) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: isodate>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from msrest<1.0.0,>=0.5.1->azureml-core~=1.34.0->azureml-synapse) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.10\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ruamel.yaml<0.17.5,>=0.15.35->azureml-core~=1.34.0->azureml-synapse) (0.2.6)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from docker<6.0.0->azureml-core~=1.34.0->azureml-synapse) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jsonpickle<3.0.0->azureml-core~=1.34.0->azureml-synapse) (4.8.1)\n",
      "Requirement already satisfied, skipping upgrade: backports.weakref in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from backports.tempfile->azureml-core~=1.34.0->azureml-synapse) (1.0.post1)\n",
      "Requirement already satisfied, skipping upgrade: jeepney>=0.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from SecretStorage<4.0.0->azureml-core~=1.34.0->azureml-synapse) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.34.0->azureml-synapse) (3.2)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests<3.0.0,>=2.19.1->azureml-core~=1.34.0->azureml-synapse) (2.0.4)\n",
      "Requirement already satisfied, skipping upgrade: plotly>=3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from autovizwidget>=0.6->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (50.3.0)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (5.0.9)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.17.2)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (3.0.20)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: backcall in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-client in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipykernel->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (6.1.12)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils~=0.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (5.1.3)\n",
      "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: nbconvert in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (5.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pyzmq>=17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (22.2.1)\n",
      "Requirement already satisfied, skipping upgrade: argon2-cffi in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (21.1.0)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: Send2Trash>=1.5.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-core>=4.6.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (4.7.1)\n",
      "Requirement already satisfied, skipping upgrade: terminado>=0.8.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.12.1)\n",
      "Collecting pykerberos<2.0.0,>=1.1.8; sys_platform != \"win32\"\n",
      "  Downloading pykerberos-1.2.1.tar.gz (24 kB)\n",
      "Requirement already satisfied, skipping upgrade: qtconsole in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jupyter>=1->hdijupyterutils<=0.16.0,>=0.15.0->azureml-synapse) (5.1.1)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-console in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jupyter>=1->hdijupyterutils<=0.16.0,>=0.15.0->azureml-synapse) (6.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure.identity>=1.4.1->azureml-synapse) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.5.0->msrest<1.0.0,>=0.5.1->azureml-core~=1.34.0->azureml-synapse) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml-core~=1.34.0->azureml-synapse) (3.10.0.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle<3.0.0->azureml-core~=1.34.0->azureml-synapse) (3.5.0)\n",
      "Requirement already satisfied, skipping upgrade: tenacity>=6.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (8.0.1)\n",
      "Collecting parso<0.8.0,>=0.7.0\n",
      "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 83.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wcwidth in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: testpath in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: bleach in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (1.4.3)\n",
      "Requirement already satisfied, skipping upgrade: defusedxml in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jinja2->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: qtpy in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from qtconsole->jupyter>=1->hdijupyterutils<=0.16.0,>=0.15.0->azureml-synapse) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>5.0.0->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (21.2.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (21.0)\n",
      "Requirement already satisfied, skipping upgrade: webencodings in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from packaging->bleach->nbconvert->notebook>=4.2->sparkmagic<=0.16.0,>=0.15.0->azureml-synapse) (2.4.7)\n",
      "Building wheels for collected packages: sparkmagic, hdijupyterutils, autovizwidget, pykerberos\n",
      "  Building wheel for sparkmagic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sparkmagic: filename=sparkmagic-0.16.0-py3-none-any.whl size=57247 sha256=c70d1085636c921576159282af83309c2daee1e7a5290ac5b04929fefff53bd3\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/69/94/d4/94d1e290dc0c3965449c6d1832ef9b8a86c81bca7bf4e632c7\n",
      "  Building wheel for hdijupyterutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.16.0-py3-none-any.whl size=7697 sha256=00d18aeb9509f3ee15a9f160042a9de6905fab922402adfb745572f8a5281196\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/21/77/5f/ed23a25516a7d1f93f04fd9953e1985a61694ed9bc3715ccae\n",
      "  Building wheel for autovizwidget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autovizwidget: filename=autovizwidget-0.19.1-py3-none-any.whl size=14544 sha256=db829c7261370a6aeab2130e9ed8f12835e4f61d263ed7b5b5f4ce9c56c18bca\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/76/cb/93/7ab4cfc3752fb2f35efddaa8332d1853d6962eefcc2b9f267d\n",
      "  Building wheel for pykerberos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pykerberos: filename=pykerberos-1.2.1-cp36-cp36m-linux_x86_64.whl size=69188 sha256=180ab9e738e9f4ce553436e06ae5726bd323560118ffc09c5eb8f5818e65facd\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/8b/04/7b/a655ef6a54543ae28ba60f30ced4a03fd4c77f4bc46b3e965a\n",
      "Successfully built sparkmagic hdijupyterutils autovizwidget pykerberos\n",
      "\u001b[31mERROR: azureml-dataprep 2.22.2 has requirement azure-identity<1.5.0,>=1.2.0, but you'll have azure-identity 1.6.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement azure-graphrbac~=0.60.0, but you'll have azure-graphrbac 0.61.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement azure-mgmt-containerregistry==3.0.0rc17, but you'll have azure-mgmt-containerregistry 8.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement azure-mgmt-keyvault==9.0.0, but you'll have azure-mgmt-keyvault 9.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement azure-mgmt-resource==18.0.0, but you'll have azure-mgmt-resource 13.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement azure-mgmt-storage~=18.0.0, but you'll have azure-mgmt-storage 11.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement azure-mgmt-synapse~=0.6.0, but you'll have azure-mgmt-synapse 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement packaging~=20.9, but you'll have packaging 21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement pytz==2019.1, but you'll have pytz 2021.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: azure-cli 2.24.0 has requirement websocket-client~=0.56.0, but you'll have websocket-client 1.2.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: msal-extensions, azure.identity, nose, hdijupyterutils, autovizwidget, pykerberos, requests-kerberos, sparkmagic, azure.mgmt.synapse, azureml-synapse, parso\n",
      "  Attempting uninstall: msal-extensions\n",
      "    Found existing installation: msal-extensions 0.2.2\n",
      "    Uninstalling msal-extensions-0.2.2:\n",
      "      Successfully uninstalled msal-extensions-0.2.2\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.2\n",
      "    Uninstalling parso-0.8.2:\n",
      "      Successfully uninstalled parso-0.8.2\n",
      "Successfully installed autovizwidget-0.19.1 azure.identity azure.mgmt.synapse azureml-synapse-1.34.0 hdijupyterutils-0.16.0 msal-extensions-0.3.0 nose-1.3.7 parso-0.7.1 pykerberos-1.2.1 requests-kerberos-0.12.0 sparkmagic-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"azureml-synapse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e7e0a",
   "metadata": {},
   "source": [
    "### Restart your kernal and refresh the web page after doing the install. \n",
    "\n",
    "This is due to the fact that package will install the line magic function `%synsapse` that will leverage.\n",
    "If you try to run the next command, and fail, you probably skipped this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c94dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::\n",
      "\n",
      "  %synapse [-s SUBSCRIPTION_ID] [-r RESOURCE_GROUP] [-w WORKSPACE_NAME]\n",
      "               [-f CONFIG_FILE] [-c COMPUTE_TARGET]\n",
      "               [--driver-memory DRIVER_MEMORY] [--driver-cores DRIVER_CORES]\n",
      "               [--executor-memory EXECUTOR_MEMORY]\n",
      "               [--executor-cores EXECUTOR_CORES] [-n NUM_EXECUTORS]\n",
      "               [-t SESSION_TIMEOUT] [--start-timeout START_TIMEOUT]\n",
      "               [-e ENVIRONMENT] [--environment-version ENVIRONMENT_VERSION]\n",
      "               [command [command ...]]\n",
      "\n",
      "Magic to execute spark remotely against a Synapse Spark pool.\n",
      "\n",
      "Sub commands:\n",
      "    start: Start a Livy session against target AML Synapse compute.\n",
      "        You can set spark config in the magic body by json format. e.g.\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            %%synapse start --compute-target synapse_compute\n",
      "            {\n",
      "                \"spark.yarn.appMasterEnv.xxx\": \"xxx\",\n",
      "                \"spark.executorEnv.xxx\": \"xxx\",\n",
      "                \"spark.yarn.maxAppAttempts\": 1\n",
      "            }\n",
      "\n",
      "    run: Run Spark code against the active session.                e.g. `%%synapse` or `%%synapse pyspark` will execute the PySpark code against the active session.                e.g. `%%synapse spark` will execute the Scala code against the active session.\n",
      "    meta: Returns the metadata of the active session.\n",
      "    stop: Stop the active session.\n",
      "\n",
      "positional arguments:\n",
      "  command               Commands to execute.\n",
      "\n",
      "optional arguments:\n",
      "  -s SUBSCRIPTION_ID, --subscription-id SUBSCRIPTION_ID\n",
      "                        Subscription id of your AML workspace.\n",
      "  -r RESOURCE_GROUP, --resource-group RESOURCE_GROUP\n",
      "                        Resource group name of your AML workspace.\n",
      "  -w WORKSPACE_NAME, --workspace-name WORKSPACE_NAME\n",
      "                        Name of your AML workspace.\n",
      "  -f CONFIG_FILE, --config-file CONFIG_FILE\n",
      "                        The path of workspace config file. Will be ignored\n",
      "                        when subscription-id, resource-group or workspace-name\n",
      "                        specified.\n",
      "  -c COMPUTE_TARGET, --compute-target COMPUTE_TARGET\n",
      "                        Name of AML Synapse pool compute.\n",
      "  --driver-memory DRIVER_MEMORY\n",
      "                        Memory size of Spark driver, default is 28g.\n",
      "  --driver-cores DRIVER_CORES\n",
      "                        Number of VCores of Spark driver, default is 4.\n",
      "  --executor-memory EXECUTOR_MEMORY\n",
      "                        Memory size of Spark executor, default is 28g.\n",
      "  --executor-cores EXECUTOR_CORES\n",
      "                        Number of VCores of Spark executor, default is 4.\n",
      "  -n NUM_EXECUTORS, --num-executors NUM_EXECUTORS\n",
      "                        Number of Spark executors, default is 2.\n",
      "  -t SESSION_TIMEOUT, --session-timeout SESSION_TIMEOUT\n",
      "                        The number of minutes of session timeout, default is\n",
      "                        30.\n",
      "  --start-timeout START_TIMEOUT\n",
      "                        The number of timeout in seconds when session is\n",
      "                        initialized, default is 300.\n",
      "  -e ENVIRONMENT, --environment ENVIRONMENT\n",
      "                        Name of the environment used for this session.\n",
      "  --environment-version ENVIRONMENT_VERSION\n",
      "                        version of the environment, will use latest version if\n",
      "                        not specified.\n"
     ]
    }
   ],
   "source": [
    "# show help\n",
    "%synapse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cbd52f",
   "metadata": {},
   "source": [
    "## Start your spark Session\n",
    "\n",
    "use Synapse compute linked to the Compute Instance's workspace with an aml envrionment.\n",
    "conda dependencies specified in the environment will be installed before the spark session started.\n",
    "\n",
    "Starting the session will take several minutes as it is installing your environment packages into your Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc557bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_compute_name=os.getenv(\"SYNAPSE_COMPUTE_NAME\", \"aml-mmsparkpool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ce1e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aml-mmsparkpool'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapse_compute_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82357382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only Conda dependencies in the environment will be used, and Python dependency specified in environment Conda dependencies is not supported in Synapse Spark pool. \n",
      "Synapse Spark pool now only supports fixed Python version, you can print \"sys.version_info\" in your script to check current Python version.\n",
      "Starting session 'aml_notebook_392851' under environment 'AzureML-Minimal', this may take several minutes .......................................................................................................... Succeeded!\n"
     ]
    }
   ],
   "source": [
    "%synapse start -c $synapse_compute_name -e AzureML-Minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec69292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Data\n",
    "\n",
    "Currently 3 types of datastores are supported with Synapse Spark\n",
    "- Blob Storage - providing credentials\n",
    "- Adls (Gen 1 & 2) - both credential & credential less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74679e6",
   "metadata": {},
   "source": [
    "## Example 1: Data loading by HDFS path\n",
    "\n",
    "### Read data from blob\n",
    "\n",
    "```python\n",
    "# setup access key or sas token\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.key.<storage account name>.blob.core.windows.net\", \"<acess key>\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.sas.<container name>.<storage account name>.blob.core.windows.net\", \"sas token\")\n",
    "\n",
    "df = spark.read.parquet(\"wasbs://<container name>@<storage account name>.blob.core.windows.net/<path>\")\n",
    "```\n",
    "\n",
    "### Read data from ADLSGen1\n",
    "\n",
    "```python\n",
    "# setup service pricinpal which has access of the data\n",
    "# If no data Credential is setup, the user identity will be used to do access control\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.adl.account.<storage account name>.oauth2.access.token.provider.type\",\"ClientCredential\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.adl.account.<storage account name>.oauth2.client.id\", \"<client id>\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.adl.account.<storage account name>.oauth2.credential\", \"<client secret>\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.adl.account.<storage account name>.oauth2.refresh.url\", \"https://login.microsoftonline.com/<tenant id>/oauth2/token\")\n",
    "\n",
    "df = spark.read.csv(\"adl://<storage account name>.azuredatalakestore.net/<path>\")\n",
    "```\n",
    "\n",
    "### Read data from ADLSGen2\n",
    "\n",
    "```python\n",
    "Read data from Adlsgen2\n",
    "\n",
    "# setup service pricinpal which has access of the data\n",
    "# If no data Credential is setup, the user identity will be used to do access control\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.auth.type.<storage account name>.dfs.core.windows.net\",\"OAuth\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth.provider.type.<storage account name>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth2.client.id.<storage account name>.dfs.core.windows.net\", \"<client id>\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth2.client.secret.<storage account name>.dfs.core.windows.net\", \"<client secret>\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.azure.account.oauth2.client.endpoint.<storage account name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<tenant id>/oauth2/token\")\n",
    "\n",
    "df = spark.read.csv(\"abfss://<container name>@<storage account>.dfs.core.windows.net/<path>\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75f717f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.34.0 to work with mm-aml-dev\n",
      "Workspace.create(name='mm-aml-dev', subscription_id='5da07161-3770-4a4b-aa43-418cbbb627cf', resource_group='mm-machine-learning-dev-rg')\n",
      "Uploading an estimated of 2 files\n",
      "Uploading ./data/train.csv\n",
      "Uploaded ./data/train.csv, 1 files out of an estimated total of 2\n",
      "Uploading ./data/test.csv\n",
      "Uploaded ./data/test.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_9d31278579ba484f9424ba37f08eee34"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n",
    "print(ws)\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "default_ds.upload_files(files=['./data/train.csv', './data/test.csv'], # Upload the diabetes csv files in /data\n",
    "                       target_path='titanic/', # Put it in a folder path in the datastore\n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ff244853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.0\n",
      "row count: 891 column count: 12\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Dataset\n",
    "import os, shutil\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core import Workspace, Dataset\n",
    "ws = Workspace.get(name='mm-aml-dev', subscription_id='5da07161-3770-4a4b-aa43-418cbbb627cf', resource_group='mm-machine-learning-dev-rg')\n",
    "#ds = Dataset.get_by_name(ws, \"<tabular dataset name>\")\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'titanic/train.csv'))\n",
    "df = tab_data_set.to_spark_dataframe()\n",
    "\n",
    "print('row count:', df.count(),'column count:', len(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1490b12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| Age|count|\n",
      "+----+-----+\n",
      "|null|   52|\n",
      "|  24|   15|\n",
      "|  27|   11|\n",
      "|  35|   11|\n",
      "|  22|   11|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "\n",
    "#from pyspark.sql.functions import col, desc\n",
    "import pyspark.sql.functions as F\n",
    "from IPython.display import display\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\")\n",
    "df.filter(F.col('Survived') == 1).groupBy('Age').count().orderBy(F.desc('count')).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6fcbc",
   "metadata": {},
   "source": [
    "# Challenge - Cleanse your data\n",
    "\n",
    "Determine which columns have NULL values and determine as a team what you would like to do with that data.  Save your resulting data to your teams' ADLS Gen2 Storage Location\n",
    "\n",
    "Helpful Articles for your team's discussion:\n",
    "[Towards Datascience](https://towardsdatascience.com/machine-learning-with-the-titanic-dataset-7f6909e58280)\n",
    "\n",
    "Potentially helpful article on [sparkbyexamples](https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/#:~:text=In%20PySpark%20DataFrame%20you%20can%20calculate%20the%20count,all%20or%20multiple%20selected%20columns%20of%20PySpark%20DataFrame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ff3c7812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 891 column count: 12\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "\n",
    "print('row count:', df.count(),'column count:', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a67d1a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b07a933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1ebad4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "unexpected EOF while parsing (<stdin>, line 8)\n",
      "  File \"<stdin>\", line 8\n",
      "    #df.isNull.fill(value=median,subset=[c])\n",
      "                                           ^\n",
      "SyntaxError: unexpected EOF while parsing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "cols = df.columns\n",
    "for c in cols:\n",
    "    value = df.agg({c: 'avg'})\n",
    "    median = value.collect()[0][0]\n",
    "    print(c, median)\n",
    "    if median != None:\n",
    "        #df = df.na.fill(value=median,subset=[c])\n",
    "        #df.isNull.fill(value=median,subset=[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1a46328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|    0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4bfa5afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6', 'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33', 'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101', 'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4', 'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35', 'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19', 'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54', 'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40', 'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44', 'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14', 'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38', 'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68', 'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48', 'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63', 'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30', 'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36', 'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42', 'C148']\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "cabins = df.select('Cabin').rdd.flatMap(lambda x: x).distinct().collect()\n",
    "cabins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "972f3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(PassengerId='1', Survived='0', Pclass='3', Name='Braund, Mr. Owen Harris', Sex='male', Age='22', SibSp='1', Parch='0', Ticket='A/5 21171', Fare='7.25', Cabin='M', Embarked='S'), Row(PassengerId='2', Survived='1', Pclass='1', Name='Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex='female', Age='38', SibSp='1', Parch='0', Ticket='PC 17599', Fare='71.2833', Cabin='C', Embarked='C'), Row(PassengerId='3', Survived='1', Pclass='3', Name='Heikkinen, Miss. Laina', Sex='female', Age='26', SibSp='0', Parch='0', Ticket='STON/O2. 3101282', Fare='7.925', Cabin='M', Embarked='S'), Row(PassengerId='4', Survived='1', Pclass='1', Name='Futrelle, Mrs. Jacques Heath (Lily May Peel)', Sex='female', Age='35', SibSp='1', Parch='0', Ticket='113803', Fare='53.1', Cabin='C', Embarked='S'), Row(PassengerId='5', Survived='0', Pclass='3', Name='Allen, Mr. William Henry', Sex='male', Age='35', SibSp='0', Parch='0', Ticket='373450', Fare='8.05', Cabin='M', Embarked='S')]\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_deck(s):\n",
    "     return 'M' if s == None else s[0]\n",
    "    \n",
    "df = df.withColumn(\"Cabin\", get_deck(\"Cabin\"))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7c973a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|    0|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "df2 = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b2fb5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+------+----+-----+--------+\n",
      "|         62|       1|     1| Icard, Miss. Amelie|female| 38|    0|    0|113572|  80|    B|    null|\n",
      "|        830|       1|     1|Stone, Mrs. Georg...|female| 62|    0|    0|113572|  80|    B|    null|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+------+----+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "df.filter(df.Embarked.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f5ea99a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|    0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "df = df.dropna(subset=[\"Embarked\"])\n",
    "\n",
    "df2 = df.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df.columns])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2fe292f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count: 889 column count: 12\n"
     ]
    }
   ],
   "source": [
    "%%synapse\n",
    "\n",
    "print('row count:', df.count(),'column count:', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42562c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
